# Enterprise LLM Governance

A comprehensive governance framework for managing and evaluating Large Language Model (LLM) prompts and responses in enterprise environments.

## Overview

This project provides tools and frameworks for:
- Prompt versioning and management
- Response evaluation (hallucination, tone, determinism, refusal, policy violations)
- Risk scoring and decision making
- Governance reporting

## Structure

- `prompts/` - Versioned prompt templates
- `inputs/` - Sample queries and test cases
- `evaluators/` - Evaluation modules for different criteria
- `engine/` - Core governance engine components
- `reports/` - Generated evaluation reports
- `config/` - Governance rules and configuration

## Usage

See `main.py` for the main entry point.

